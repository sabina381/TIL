{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "wn8RchLQNWUy"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D_WkQJmhxWue"
      },
      "source": [
        "# 함수 미분"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "EshPEPycNaeO"
      },
      "outputs": [],
      "source": [
        "# np를 이용한 함수\n",
        "def my_derivative(t):\n",
        "    return (4*t**3 + t**4 + np.cos(np.exp(t)))*np.exp(t)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WUtY4mvVNx3J",
        "outputId": "5d75d7ea-32bd-4399-d13c-74142b0a0fcd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "11.113059409339991"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "my_derivative(1.0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EujATXzTONT8",
        "outputId": "061b5329-ebaf-421c-d74a-1c4378f84a90"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([2.7183])"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# torch를 이용한 함수와 auto derivative\n",
        "x = torch.tensor([1.], requires_grad=True) # tenor안에 실수형 (int 오류남)\n",
        "y = torch.exp(x)\n",
        "\n",
        "y.backward()\n",
        "x.grad"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M4cW5pO5xqLB"
      },
      "source": [
        "- 미분하는 문자: 대입할 값을 미리 지정해줘야 하고, `requires_grad = True`를 꼭 지정해줘야 이 문자로 미분할 것이라고 알려주는 것.\n",
        "- y는 x에 연결되어있는 torch 객체로, x의 연산 기능을 그대로 가져오게된다.\n",
        "\n",
        "> `.backward()`를 호출하면,\n",
        "> - 출력 텐서에서 시작하여 입력 방향으로 그래디언트를 계산한다.\n",
        "> - 계산 그래프를 따라 각 연산의 편미분을 계산하고, 체인 룰을 적용하여 전체 그래디언트를 구한다.\n",
        "> - 계산된 그래디언트는 `requires_grad=True`로 설정된 각 텐서의 `.grad` 속성에 누적된다.\n",
        "\n",
        "> `x.grad`에 값이 저장되는 원리\n",
        "> - `backward()` 호출 시, PyTorch는 손실 함수에서 x까지의 모든 경로를 따라 편미분을 계산한다.\n",
        "> - PyTorch의 autograd 엔진이 각 연산의 그래디언트를 자동으로 계산한다.\n",
        "> - 계산된 최종 그래디언트는 `x.grad`에 저장한다. (함수를 x에 대해 미분한 결과값)\n",
        "> -  그래디언트는 누적되기 때문에, `.backward()`를 여러번 호출할 경우, `.zero_()`를 호출해 x의 그래디언트 값들을 0으로 초기화햐야한다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uR_k_MWmOga1",
        "outputId": "58a48665-084c-4895-a830-9cac8d984669"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([11.1131])"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 자동으로 체인룰을 적용하여 계산\n",
        "t = torch.tensor([1.0], requires_grad=True)\n",
        "u = [t**2, torch.exp(t)]\n",
        "z = u[0]**2 * u[1] + torch.sin(u[1])\n",
        "\n",
        "z.backward()\n",
        "t.grad"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cCffgYZTPc7z",
        "outputId": "e5385f53-e6f1-43b0-bb3d-70faf6803d5c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([11.1131])"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "t = torch.tensor([1.0], requires_grad=True)\n",
        "u = torch.stack([t**2, torch.exp(t)])\n",
        "z = u[0]**2 * u[1] + torch.sin(u[1])\n",
        "\n",
        "z.backward()\n",
        "t.grad"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yXkD6XHb0WUE"
      },
      "source": [
        "- `torch.stack()`메소드를 사용하면 차원을 유지한 채로 결합할 수 있다.\n",
        "    - 새로운 차원을 생성하여 그 차원을 따라 텐서들을 쌓는다.\n",
        "\n",
        "> - 모든 입력 텐서의 shape이 정확히 동일해야 한다.\n",
        "> - 결과 텐서의 차원 수는 입력 텐서들보다 1만큼 더 많아진다.\n",
        "\n",
        "- `.concatenate()`메서드는 입력된 기존 tensor들의 차원을 따라서 결합하기 때문에 $X$와 $Y$를 $[20,1]$차원으로 만든 후에 결합해야한다.\n",
        "\n",
        "> - 모든 입력 텐서들의 차원이 동일해야한다.\n",
        "> - 결과 텐서의 차원 수는 입력 텐서들과 동일하다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HiVuCfWCSOW0",
        "outputId": "0f098f82-a33d-4710-cec1-8b69f8cf3461"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([11.1131])"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "t = torch.tensor([1.0], requires_grad=True)\n",
        "\n",
        "def g(T):\n",
        "    return torch.stack([T**2, torch.exp(T)])\n",
        "\n",
        "def f(U):\n",
        "    return U[0]**2 * U[1] + torch.sin(U[1])\n",
        "\n",
        "z = f(g(t))\n",
        "z.backward()\n",
        "t.grad"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2GaLJBCBS89a",
        "outputId": "069bf30b-0b75-439f-c483-837cc92a37db"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[30.        , 21.        ],\n",
              "       [ 7.68136229,  4.80085143]])"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 행렬로 계산\n",
        "u = np.array([1,2])\n",
        "Dg = np.array([[1,1],\n",
        "               [u[1], u[0]]])\n",
        "\n",
        "x = np.array([u[0]+u[1], u[0]*u[1]])\n",
        "Df = np.array([[2*x[0]*x[1], x[0]**2],\n",
        "               [x[1]*np.cos(x[0]*x[1]), x[0]*np.cos(x[0]*x[1])]])\n",
        "Df@Dg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QWcJB7J0XJkX",
        "outputId": "1f369656-70b8-4c74-a449-2b45a22e0bf2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([30., 21.])"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# torch로 계산\n",
        "u = torch.tensor([1.,2.], requires_grad=True)\n",
        "x = torch.stack([u[0]+u[1], u[0]*u[1]]) # stack으로 쌓으면 차원 유지 가능\n",
        "z1 = x[0]**2 *x[1]\n",
        "z1.backward()\n",
        "u.grad"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S8hswUGR3tLF",
        "outputId": "c234433b-49e6-4a23-ec9e-749eb6ce2986"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([7.6814, 4.8009])"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "u = torch.tensor([1.,2.], requires_grad=True)\n",
        "x = torch.stack([u[0]+u[1], u[0]*u[1]])\n",
        "z2 = torch.sin(x[0]*x[1])\n",
        "z2.backward()\n",
        "u.grad"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ozRbfKEzYm_d"
      },
      "source": [
        "스칼라값이 아닌 벡터에 대해 `.backward()` 할 경우 다음과 같은 에러 발생\n",
        "\n",
        "```Error: grad can be implicitly created only for scalar outputs```\n",
        "\n",
        "> `torch`의 `backward()` 메서드의 제약 사항 두가지가 있다.\n",
        "> 1. **스칼라값(단일 요소를 가진 텐서)**에 대해서만 호출할 수 있다.\n",
        "> 2. 출력이 스칼라가 아닌 경우, **gradient 인자를 명시적으로 제공**해야 한다. 즉, 각 출력 요소에 대한 기울기의 공간을 제시해주어야 한다.\n",
        "\n",
        "- 1. 조건을 만족시켜 해결하기 위해, y(함수)에 `.sum()`이나 `.mean()` 등의 함수를 적용해 스칼라값으로 만들어준 뒤에 backward()한다.\n",
        "- 2. 조건을 만족시켜 해결하기 위해, 다음과 같이 backward 메서드에 gradient 인자를 명시적으로 제공한다.\n",
        "```\n",
        "y.backward(gradient=torch.ones_like(y))\n",
        "```\n",
        "> `torch`의 `backward()` 메서드의 제약 사항 두가지가 있다.\n",
        "> 1. **스칼라값(단일 요소를 가진 텐서)**에 대해서만 호출할 수 있다.\n",
        "> 2. 출력이 스칼라가 아닌 경우, **gradient 인자를 명시적으로 제공**해야 한다. 즉, 각 출력 요소에 대한 기울기의 공간을 제시해주어야 한다.\n",
        "\n",
        "- 1. 조건을 만족시켜 해결하기 위해, 이 문제의 경우에는 두 함수에 대해 따로따로 미분해준다. (위의 코드블록)\n",
        "- 2. 조건을 만족시켜 해결하기 위해, 다음과 같이 backward 메서드에 gradient 인자를 명시적으로 제공한다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LKnOZt5Y232R",
        "outputId": "9c643e70-6d54-4008-9806-cecabbd49efb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([37.6814, 25.8008])"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "u = torch.tensor([1.,2.], requires_grad=True)\n",
        "x = torch.stack([u[0]+u[1], u[0]*u[1]]) # stack으로 쌓으면 차원 유지 가능\n",
        "f = torch.stack([x[0]**2 *x[1], torch.sin(x[0]*x[1])])\n",
        "f.backward(gradient=torch.ones_like(f))\n",
        "u.grad"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1yv77_MtZNnE",
        "outputId": "003f5863-f831-4065-a921-c3e14ea96f55"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([15.3627,  9.6017])"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x = torch.stack([u[0]+u[1], u[0]*u[1]]) # stack으로 쌓으면 차원 유지 가능\n",
        "z1 = x[0]**2 *x[1]\n",
        "z2 = torch.sin(x[0]*x[1])\n",
        "z2.backward()\n",
        "u.grad"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qblsQKTeZOgC"
      },
      "source": [
        "- 이런식으로 backward를 여러번 하면 `u.grad`에 계속 값을 누적함"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8h_6X6q6Xsa0",
        "outputId": "cd86d241-b2a6-4f4b-ee4c-3b554a8466f5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([7.6814, 4.8009])"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 해결 방법\n",
        "x = torch.stack([u[0]+u[1], u[0]*u[1]]) # stack으로 쌓으면 차원 안변함\n",
        "z1 = x[0]**2 *x[1]\n",
        "z2 = torch.sin(x[0]*x[1])\n",
        "z2.zero_() # 그래디언트 초기화\n",
        "z2.backward()\n",
        "u.grad"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pAu0spz54VXD"
      },
      "source": [
        "# 차원 문제 (broadcasting 이슈)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "IE1rjAZR4TIm"
      },
      "outputs": [],
      "source": [
        "X=torch.tensor([-3.0000e+00, -2.7000e+00, -2.4000e+00, -2.1000e+00, -1.8000e+00,\n",
        "        -1.5000e+00, -1.2000e+00, -9.0000e-01, -6.0000e-01, -3.0000e-01,\n",
        "        -2.3842e-08,  3.0000e-01,  6.0000e-01,  9.0000e-01,  1.2000e+00,\n",
        "         1.5000e+00,  1.8000e+00,  2.1000e+00,  2.4000e+00,  2.7000e+00])\n",
        "Y = torch.tensor([-7.1452, -5.4253, -5.1977, -3.6225, -3.8022, -4.4101, -4.6622, -3.1932,\n",
        "        -1.7325, -1.8879, -1.0742, -0.2320,  1.8226,  1.5453, -1.5535,  0.8857,\n",
        "         1.7537,  3.1607,  1.8912,  4.0895])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hhSw1lPKZgI3",
        "outputId": "a08938ed-3d62-436a-fc33-6d63e70ae72b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[  94.5796],\n",
              "        [-159.6058]])"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "BETA = torch.tensor([[1.0], [0.5]], requires_grad=True) # shape: [2,1]\n",
        "ones = torch.ones([20])\n",
        "X_ = torch.stack([ones, X], axis=1) # shape: [20,2]\n",
        "Y_ = Y.reshape(-1,1) # shape: [20,1] (Y: [20])\n",
        "L = torch.sum((Y_ - X_ @ BETA)**2) # Y로 하면 잘못 계산됨\n",
        "L.backward()\n",
        "BETA.grad"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HgIuGiGuVliI",
        "outputId": "952cfcb7-b981-43c7-da9b-2f12237031db"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[  94.5796],\n",
              "        [-159.6058]])"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "beta = torch.tensor([1.0, 0.5], requires_grad=True) # shape: [2]\n",
        "beta_ = beta.reshape(2,-1) # shape: [2,1]\n",
        "ones = torch.ones([20])\n",
        "X_ = torch.stack([ones, X], axis=1) # shape: [20,2]\n",
        "Y_ = Y.reshape(-1,1) # shape: [20,1] (Y: [20])\n",
        "L = torch.sum((Y_ - X_ @ beta_)**2) # beta로 하면 잘못 계산됨\n",
        "L.zero_()\n",
        "L.backward()\n",
        "BETA.grad"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 기타\n",
        "\n",
        "- Matplotlib의 plt에서 허용되는 데이터 형식\n",
        "    - `np.array`, `list`, `pandas.Series`, `pandas.DataFrame`\n",
        "    - `torch.Tensor`는 허용되지 않는다\n",
        "\n",
        "- `detach()` 메소드는 현재 Tensor 객체와 동일한 데이터를 가지지만 연산에서 분리된 새로운 Tensor 객체를 생성한다. 일반적으로 Tensor 객체를 다른 Tensor 객체로 변환하고자 할 때 사용된다.\n",
        "-  `requires_grad=True`를 설정한 torch.Tensor은 연산이 포함되어있다. 그래서 그냥은 numpy로 변환할 수 없기 때문에 `detach()`를 꼭 해줘야한다."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
